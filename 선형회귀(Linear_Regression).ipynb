{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP42K03+UMsp7oiqPs6uU9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngmi-Park/pytorch-study/blob/main/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80(Linear_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 선형 회귀(Linear Regression)\n",
        "\n",
        "공부한 시간(x)과 점수(y)에 대한 상관관계로 선형 회귀\n",
        "*   1시간 → 2점\n",
        "*   2시간 → 4점\n",
        "*  3시간 → 6점\n",
        "일 때, 4시간을 공부한다면 몇 점을 맞을 수 있을까?\n"
      ],
      "metadata": {
        "id": "qkGhIcSOFO9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 데이터셋 구성\n",
        "\n",
        "- 훈련 데이터셋(training dataset): 예측을 위해 모델을 학습하는데 사용하는 데이터\n",
        "\n",
        "- 테스트 데이터셋(test dataset): 학습이 끝난 후, 모델이 얼마나 잘 작동하는지 판별하는 데이터셋"
      ],
      "metadata": {
        "id": "oe9PCSsMGPT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 가설(Hypothesis) 수립\n",
        "\n",
        "머신 러닝에서 가설\n",
        "- 임의로 추측해서 세우는 식\n",
        "- 경험적으로 알고 있는 식\n",
        "- 맞는 가설이 아니라고 판단되면 계속 수정하는 식\n",
        "\n",
        "\n",
        "#### 선형회귀\n",
        "학습 데이터와 가장 잘 맞는 하나의 직선 방정식을 찾는 것이다.\n",
        "$$ y = Wx+b $$\n",
        "가설 H를 따서 $H(x) = Wx+b$로 표현하기도 한다.\n",
        "- $W$: Weight, 가중치(기울기)\n",
        "- $b$: bias, 편향(절편)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao-hWXbxGqc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 비용 함수(Cost function)\n",
        "\n",
        "비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)\n",
        "\n",
        "이때, 모든 데이터에 대해서 오차(실제값-예측값)를 더하면 음수와 양수가 더해지므로 절대적인 오차의 크기를 측정할 수 없다.\n",
        "따라서, 오차들의 제곱의 합을 구하고 데이터 개수인 n으로 나누어 주면 오차의 제곱합에 대한 평균을 구할 수 있다.\n",
        "\n",
        "이를 **평균 제곱 오차(Mean Squared Error, MSE)**라고 한다. \n",
        "\n",
        "평균 제곱 오차는 회귀 문제에서 적절한 W\n",
        "와 b를 찾기위해서 최적화된 식이다. 그 이유는 평균 제곱 오차의 값을 최소값으로 만드는 W\n",
        "와 b를 찾아내는 것이 가장 훈련 데이터를 잘 반영한 직선을 찾아내는 일이기 때문이다.\n",
        "\n",
        "$$cost(W, b) = {1 \\over n} \\sum^n_{i=1}[y^{(i)}-H(x^{(i)})]^2   $$\n",
        "\n",
        "즉, $Cost(W,b)$를 최소화 하는 W, b를 찾으면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있다."
      ],
      "metadata": {
        "id": "IWfFe5ecH03T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 경사 하강법(Gradient Descent) - 옵티마이저\n",
        "\n",
        "비용 함수(Cost Function)의 값을 최소로 하는 \n",
        "$W$와 $b$를 찾는 방법, 최적화 알고리즘이라고도 하는 옵티마이저(Optimizer)이다.\n",
        "- 머신 러닝에서 학습(training): 옵티마이저 알고리즘을 통해 적절한 \n",
        "W와 b를 찾아내는 과정\n",
        "- 경사 하강법: 가장 기본적인 옵티마이저 알고리즘\n",
        "\n",
        "편항 $b$없이 $W$만으로 설명하면, 기울기(가중치) $W$가 무한대로 커지면 cost 값도 무한대로 커지고, $W$가 무한대로 작아져도 cost값이 무한대로 커진다.\n",
        "cost가 가장 최솟값을 가지게 하는 $W$를 찾아야 하므로, 맨 아래 볼록한 부분의 $W$값을 찾아야한다.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://wikidocs.net/images/page/21670/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.PNG\"></p>\n",
        "\n",
        "임의의 초기값 $W$에서 시작 → cost function 미분 → 현재 $W$의 접선의 기울기를 구함 → 접선의 기울기가 낮은 방향으로 $W$값 갱신 반복\n",
        "\n",
        "<p align=\"center\"><img src=\"https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG\n",
        "\"></p>\n",
        "\n",
        "- 기울기가 음수일 때: $W$ 값이 증가\n",
        "$$W := W - \\alpha \\times (음수기울기)= W+\\alpha \\times (양수기울기)$$\n",
        "\n",
        "- 기울기가 양수일 때: $W$ 값이 감소\n",
        "$$W := W - \\alpha \\times (양수기울기)$$\n",
        "\n",
        "두 경우 모두 접선의 기울기가 0인 방향으로 $W$조정한다.\n",
        "$$W := W - \\alpha{∂ \\over ∂W } cost(W)$$\n",
        "\n",
        "### 학습률(learning rate)\n",
        "학습률은 위의 식에서 $\\alpha$를 말한다.\n",
        "$W$, $b$값을 변경할 때, 얼마나 크게 변경할지를 결정하는 값이다. 적당한 $\\alpha$를 찾는 것도 중요하다. \n",
        "\n",
        "- 학습률이 지나치게 큰 경우:값이 발산\n",
        "- 학습률이 지나치게 작은 경우: 학습 속도 저하\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념이다. 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법이다.*\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTrH4kAlM6V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch로 구현한 선형 회귀 \n",
        "\n",
        "## 직접 정의해서 구현하기\n",
        "\n",
        "### 1. 기본 세팅"
      ],
      "metadata": {
        "id": "zGhq3nS5nhQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim # optimizer\n",
        "\n",
        "torch.manual_seed(1) # 재실행해도 같은 결과가 나오도록 랜덤 시드 설정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpdeORJGGpVd",
        "outputId": "e7b45bd6-498f-43fe-8d64-25d7539d9b63"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7faddc1a8470>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 변수 선언"
      ],
      "metadata": {
        "id": "OPmSgFNnn--X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bY1_WyXPFMWI"
      },
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]]) # 공부한 시간\n",
        "y_train = torch.FloatTensor([[2],[4],[6]]) # 점수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifpky7nSnf9B",
        "outputId": "530eda46-2250-44c0-a204-4a2bf5a491e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 가중치, 편향 초기화\n",
        "\n",
        "선형 회귀는 학습 데이터와가장 잘 맞는 하나의 직선 $y=Wx+b$을 찾는 일이다.\n",
        "\n",
        "즉, 직선을 정희하는 $W$와 $b$의 값을 찾는 것이 목표이다.\n",
        "\n",
        "- requires_grad=True 인자: 학습을 통해 계속 값이 변경되는 변수임을 의미"
      ],
      "metadata": {
        "id": "B8mcnGFhoQpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치와 편향을 0으로 초기화 하고 학습을 통해 값이 변경되는 변수임을 명시\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "print(W)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYicO4DcoR6E",
        "outputId": "f3e1cfcd-996f-4b7e-eb1f-cad8dcf7381b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.], requires_grad=True)\n",
            "tensor([0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 가설 세우기\n",
        "$$H(x) = Wx + b$$\n",
        "파이토치 코드 상으로 직선 방정식에 해당하는 가설을 선언한다."
      ],
      "metadata": {
        "id": "5dbWMsu2pLCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis = x_train * W + b  # hypothesis\n",
        "print(hypothesis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl5lm9nNoLZS",
        "outputId": "0e7686e9-95e9-49df-bfe7-e75d33d8ba96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 비용 함수 선언하기\n",
        "\n",
        "파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언한다.\n",
        "\n",
        "$$cost(W, b) = {1 \\over n} \\sum^n_{i=1}[y^{(i)}-H(x^{(i)})]^2   $$\n"
      ],
      "metadata": {
        "id": "Cb55flOtpewI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = torch.mean((hypothesis - y_train) ** 2)# cost function\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ9MQNCGpaZj",
        "outputId": "451f1da8-55ae-45a4-db6e-5237a6c3ae05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 경사하강법 구현하기\n",
        "- SGD: 경사 하강법의 일종\n",
        "- lr: 학습률(learning rate)를 의미\n",
        "\n",
        "학습 대상인 $W$와 $b$가 SGD의 입력으로 주어진다."
      ],
      "metadata": {
        "id": "nWTXF3JyqBpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD([W, b], lr=0.01)"
      ],
      "metadata": {
        "id": "NTsZGOzrp7Q8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer.zero_grad()\n",
        "- 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화한다. \n",
        "- 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다. \n",
        "\n",
        "cost.backward() \n",
        "- 가중치 W와 편향 b에 대한 기울기가 계산된다. \n",
        "\n",
        "opimizer.step() \n",
        "- 경사 하강법 최적화 함수\n",
        "- 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ICke8QPfqoUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad() # gradient를 0으로 초기화\n",
        "cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
        "optimizer.step() # W와b를 업데이트"
      ],
      "metadata": {
        "id": "9tNjDU8Cq_yN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 전체 코드\n",
        "\n"
      ],
      "metadata": {
        "id": "jvkywwdVrWhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 선언\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\n",
        "\n",
        "# 모델 가중치 편향 0으로 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=0.01)\n",
        "\n",
        "nb_epochs = 2000 # 에폭, 원하는 만큼 경사 하강법 반복\n",
        "for epoch in range(nb_epochs+1):\n",
        "  # H(X) 계산\n",
        "  hypothesis = x_train * W + b # 선형회귀 직선식\n",
        "\n",
        "  # cost 계산, 평균 제곱 오차\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # gradient를 0으로 초기화\n",
        "  cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
        "  optimizer.step() # W와b를 업데이트\n",
        "\n",
        "  # 100 epoch마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch:4d}/{nb_epochs} W: {W.item():.3f}, b: {b.item():.3f} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_IdYe4SrNJv",
        "outputId": "cc6d8a5d-6cd3-44b3-eec2-d5aa056a8c5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종 훈련 결과를 보면 최적의 기울기 $W$\n",
        "는 2에 가깝고, $b$는 0에 가까운 것을 볼 수 있다.\n",
        "\n",
        "현재 훈련 데이터가 x_train은 [[1], [2], [3]]이고 y_train은 [[2], [4], [6]]인 것을 감안하면 실제 정답은 $W$가 2이고, $b$가 0인 \n",
        "이므로 거의 정답을 찾았다고 볼 수 있다."
      ],
      "metadata": {
        "id": "RItk8mp5unCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Module로 구현하는 선형 회귀\n",
        "\n",
        "파이토치에서 이미 구현되어져 제공되고 있는 함수들을 불러와서 더 쉽게 선형 회귀 모델을 구현할 수 있다.\n",
        "\n",
        "- nn.Linear(input_dim, output_dim): 선형 회귀 모델\n",
        "- nn.functional.mse_loss(prediction, y_train): 평균 제곱 오차"
      ],
      "metadata": {
        "id": "Pex2TIJ6xKMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 선형 회귀 모델 구현하기"
      ],
      "metadata": {
        "id": "Abgexl_uyej-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1) # 랜덤 시드 고정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJabJSSerrLb",
        "outputId": "1fdd5f12-852e-4c81-b296-a32964810748"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7faddc1a8470>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정의\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "metadata": {
        "id": "ljQx6oBRyqAH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀 모델 구현\n",
        "- nn.Linear()는 입력의 차원, 출력의 차원을 인수로 받는다.\n",
        "- (1, 1): 하나의 입력 x에 대해 하나의 출력 y를 가진다."
      ],
      "metadata": {
        "id": "lYv6EzPvzKFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(1,1) #input_dim, output_dim"
      ],
      "metadata": {
        "id": "9vpEbITAzxWC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- model에는 가중치 W와 편향 b가 저장되어져 있다. \n",
        "- model.parameters()라는 함수로 가중치와 편향을 불러올 수 있다.\n",
        "- 첫번째 값이 W, 두번째 값이 b이다.\n",
        "- 초기에는 랜덤 초기화 되어 있고, 두 값 모두 학습 대상이므로 requires_grad=True 설정 되어있다."
      ],
      "metadata": {
        "id": "qxBa-lko0Gsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1EXmPGsy1Ir",
        "outputId": "e2a1f7ff-fa3c-4ae6-b859-962433b5404b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4414], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "옵티마이저 정의\n",
        "- model.parameters()로 W와 b를 전달한다."
      ],
      "metadata": {
        "id": "P363uKb30ijY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "i-7mCahGz-Vx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  prediction = model(x_train) # 예측값 H(x)계산, forward 연산\n",
        "  cost = F.mse_loss(prediction, y_train)  # cost 계산\n",
        "  \n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # gradient를 0으로 초기화\n",
        "  cost.backward() # 비용 함수를 미분하여 gradient 계산, backward 연산\n",
        "  optimizer.step() # W와 b를 업데이트\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02AIW8dY0wjw",
        "outputId": "6cae1fd3-3782-4925-c9a1-08043e1ab609"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 13.103541\n",
            "Epoch  100/2000 Cost: 0.002791\n",
            "Epoch  200/2000 Cost: 0.001724\n",
            "Epoch  300/2000 Cost: 0.001066\n",
            "Epoch  400/2000 Cost: 0.000658\n",
            "Epoch  500/2000 Cost: 0.000407\n",
            "Epoch  600/2000 Cost: 0.000251\n",
            "Epoch  700/2000 Cost: 0.000155\n",
            "Epoch  800/2000 Cost: 0.000096\n",
            "Epoch  900/2000 Cost: 0.000059\n",
            "Epoch 1000/2000 Cost: 0.000037\n",
            "Epoch 1100/2000 Cost: 0.000023\n",
            "Epoch 1200/2000 Cost: 0.000014\n",
            "Epoch 1300/2000 Cost: 0.000009\n",
            "Epoch 1400/2000 Cost: 0.000005\n",
            "Epoch 1500/2000 Cost: 0.000003\n",
            "Epoch 1600/2000 Cost: 0.000002\n",
            "Epoch 1700/2000 Cost: 0.000001\n",
            "Epoch 1800/2000 Cost: 0.000001\n",
            "Epoch 1900/2000 Cost: 0.000000\n",
            "Epoch 2000/2000 Cost: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_var =  torch.FloatTensor([[4.0]]) # 임의의 입력 4를 선언\n",
        "pred_y = model(new_var) # forward 연산, 입력 4에 대해 예측값 y를 pred_y에 저장\n",
        "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
        "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sT141pq1wq4",
        "outputId": "42e95949-8d8a-4bf0-a87a-17ef21a819b5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('학습 후의 W와 b의 값', list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm2KlSxu1wKs",
        "outputId": "2e20e259-fd75-41c1-a8cd-4ae63259da69"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 후의 W와 b의 값 [Parameter containing:\n",
            "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0014], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "W의 값이 2에 가깝고, b의 값이 0에 가까운 것을 볼 수 있다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O4p3Skvc1rTq"
      }
    }
  ]
}