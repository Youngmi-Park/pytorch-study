{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjHwOH3NEeLWF3Lah4FBWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngmi-Park/pytorch-study/blob/main/%EB%8B%A4%EC%A4%91_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80(Multivariable_Linear_regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다중 선형 회귀\n",
        "\n",
        "- 단순 선형 회귀(Simple Linear Regression): x가 1개인 선형 회귀\n",
        "- 다중 선형 회귀(Multivariable Linear Regression): 다수의 x로 부터 y를 에측\n",
        "\n",
        "### 1. 데이터에 대한 이해\n",
        "<p align=\"center\"> <img src=\"https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG\"></p>\n",
        "\n",
        "$$H(x) = w_1x_1 + w_2+x_2 + w_3x_3 +b$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      w_{1} \\\\\n",
        "      w_{2} \\\\\n",
        "      w_{3} \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "+\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      b \\\\\n",
        "      b \\\\\n",
        "      b \\\\\n",
        "      b \\\\\n",
        "      b \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        " \\ =\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3} + b\\ \\\\\n",
        "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3} + b\\ \\\\\n",
        "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3} + b\\ \\\\\n",
        "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3} + b\\ \\\\\n",
        "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3} + b\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "  $$\n",
        "\n",
        "### 2. PyTorch로 구현하기"
      ],
      "metadata": {
        "id": "3rPlzYF2jJ8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBJzENWtjBan",
        "outputId": "2832393c-2b2c-4120-c952-ffab6c03646c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe2c08ea3f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # 재실행결과를 동일하게 하기 위한 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import W_OK\n",
        "# 훈련 데이터 선언, 행렬 연산을 고려하여 구현\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 80],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]]) # (5, 3) \n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]]) # (5, 1)\n",
        "\n",
        "# 가중치 w(3개 필요), 편향 b 선언\n",
        "W = torch.zeros((3, 1), requires_grad=True) # 좌측 행렬의 열의 크기 = 우측 행렬의 행의 크기\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# 가설, 비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 1,000회 반복합니다.\n",
        "optimizer = optim.SGD([W, b], lr=1e-5) # optimizer\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = x_train.matmul(W) + b # 가설 H(x), matmul 행렬곱, 편향 b는 브로드캐스팅되어 더해짐\n",
        "  cost = torch.mean((hypothesis - y_train)**2)# cost 계산, 평균 제곱 오차\n",
        "\n",
        "  # cost로 H(x)개선\n",
        "  optimizer.zero_grad() # gradient를 0으로 초기화\n",
        "  cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
        "  optimizer.step() # W(w1, w2, w3)와b를 업데이트\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "      print('Epoch {:4d}/{} hypothesis: {} W: {} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, hypothesis.squeeze().detach(), W.squeeze().detach(), cost.item()\n",
        "      ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bipXtxWGj_ZX",
        "outputId": "4d01957a-d1f3-4435-8bdc-93ff308306b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1000 hypothesis: tensor([0., 0., 0., 0., 0.]) W: tensor([0.2940, 0.2936, 0.2902]) Cost: 29661.800781\n",
            "Epoch  100/1000 hypothesis: tensor([154.0433, 185.0925, 175.8312, 198.5701, 141.2221]) W: tensor([0.6882, 0.6809, 0.6577]) Cost: 5.754573\n",
            "Epoch  200/1000 hypothesis: tensor([154.0278, 185.0649, 175.9335, 198.5128, 141.2284]) W: tensor([0.6974, 0.6837, 0.6455]) Cost: 5.512386\n",
            "Epoch  300/1000 hypothesis: tensor([154.0120, 185.0385, 176.0329, 198.4569, 141.2353]) W: tensor([0.7065, 0.6863, 0.6336]) Cost: 5.281667\n",
            "Epoch  400/1000 hypothesis: tensor([153.9960, 185.0133, 176.1295, 198.4022, 141.2426]) W: tensor([0.7155, 0.6888, 0.6221]) Cost: 5.061907\n",
            "Epoch  500/1000 hypothesis: tensor([153.9797, 184.9892, 176.2233, 198.3488, 141.2504]) W: tensor([0.7243, 0.6911, 0.6108]) Cost: 4.852424\n",
            "Epoch  600/1000 hypothesis: tensor([153.9632, 184.9662, 176.3143, 198.2966, 141.2586]) W: tensor([0.7330, 0.6932, 0.5999]) Cost: 4.652731\n",
            "Epoch  700/1000 hypothesis: tensor([153.9465, 184.9442, 176.4029, 198.2456, 141.2672]) W: tensor([0.7415, 0.6952, 0.5892]) Cost: 4.462265\n",
            "Epoch  800/1000 hypothesis: tensor([153.9296, 184.9232, 176.4888, 198.1958, 141.2762]) W: tensor([0.7499, 0.6971, 0.5788]) Cost: 4.280604\n",
            "Epoch  900/1000 hypothesis: tensor([153.9126, 184.9032, 176.5724, 198.1471, 141.2855]) W: tensor([0.7581, 0.6988, 0.5687]) Cost: 4.107261\n",
            "Epoch 1000/1000 hypothesis: tensor([153.8955, 184.8841, 176.6536, 198.0995, 141.2951]) W: tensor([0.7663, 0.7004, 0.5589]) Cost: 3.941853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Module로 다중선형 회귀 구현하기\n",
        "\n",
        "nn.Linear()와 nn.functional.mse_loss()로 단순 선형 회귀를 구현할 때 처럼 다중 선형 회귀를 구현할 수 있다.\n",
        "\n",
        "달라지는 점은 nn.Linear()의 입력 차원과 출력 차원이다.\n",
        "\n",
        "(자세한 설명은 생략)"
      ],
      "metadata": {
        "id": "YPxncsAwpHZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "model = nn.Linear(3,1)"
      ],
      "metadata": {
        "id": "wA6GpF1zkCP3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}